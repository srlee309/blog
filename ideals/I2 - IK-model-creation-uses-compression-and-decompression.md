---
title: 'Model creation uses compression and decompression'
description: 'Model creation uses compression and decompression'
tags: Productivity
publishedDate: 11/4/2020
published: false
slugs:
    - ___UNPUBLISHED___kdqqtgj6_puPT211AeVXpTpXtkRK1138Mcr2bTIB9

---
A common assumption that is made is that we observe the world and then build models from those observations. That is, that models are secondary to the process of perception. The truth is that models are primary. We view the world primarily through our models and use our observations to align and update the models.

Perception is model driven and composed of two interwoven processes: compression of observed instanced into models and decompression of those models in order to fill in and assume information:

- compression: the human brain has the extraordinary ability to be able to pick out patterns and identify recurrences despite variations. It achieves this not by modeling instances, but by instead through modeling the patterns between observed instances. That is modelling what is essential. The reason it does is that the brains capacity is limited and these patterns are the most useful. Patterns are essentially predictions, for example black clouds preceding rain. The modeling of these patterns is called compression and is captured by machine learning techniques that move from instances (e.g., pictures of cats) to general regularities (e.g., what a cat looks like), much like inductive inference. Compression allows us to see things as the same across multiple instances that have extremely different circumstances.
- decompression¹: a second extraordinary ability of the human brain is the ability to decompress or to flesh out our models just-in-time. When we look out at the world, we are not just seeing it we are also constructing it². We are filling in and assuming information about the world. We are presuming relationships between the things that are in front of us and we are presuming that they are related to our pre-existing heavily compressed and prototypical models. If we see a dog for less than a fraction of a second, we also, in a way, see a tail, four legs and fur even though we might not actually have physically seen these things. Decompression is the opposite of compression as it moves from regularities to instances, much like deductive inference. Decompression is about drawing upon previously compressed models as well as the overall context and neural excitation for example emotions, desires and thoughts, in order to flesh out or fill in the missing details in the compressed model.

The goal of the decompression is to deduce implicit information in the compressed representation. Even as infants, we have extraordinary decompression abilities. If we see a tower of blocks, we instantly know whether it is stable or likely to topple. We can quickly recognize faces and guess based on facial expressions what those people are feeling. In situation after situation, we can rapidly jump from particular facts to generalizations that help us understand new facts and situations.

If compression is stripping things down to their patterns, decompression is using those patterns to both simplify and extend perception. Decompression is powerful because if you have a model that matches the world, then you don’t need to view the world itself you can just view the model. Being able to draw upon or fall back to pre-existing models allows us to perceive things in a holistic way that goes beyond the data that is currently available . For example, it allows us to know where a ball will fall when we throw it. Models and predictions greatly simplify perception as it reduces the task of building a model from observing reality to viewing the model and observing reality for the purpose of updating the model.

Model based perception grants us extraordinary perceptual capabilities; however, it does have its flaws. It is entirely dependent on our pre-existing and potentially flawed models . With model based perception, what we see is likely to not be what is actually there, but a projection that is largely fleshed out from our pre-existing models.

Perception is continuous cycle of decompression and compression which means that models evolve overtime. They take on aspects of other models when they are decompressed and move further out of conscious awareness when they are compressed. This leads to the subsumption of models in a process called chunking³.

Chunking is when we parse information into chunks that are more memorable and easier to process than the individual models of which they’re composed. As an example, a child learning letters keeps a forceful distinction between seeing a shape and seeing a letter: the first causes the second. But very soon, the child cannot distinguish the cause from the effect: they can no longer see the shape without seeing the letter. The shape and letter becomes a chunk. This kind of chunking process plays a large part in the acquisition of all kinds of expert knowledge. Experts are able to perform the way they do because they have over a long period of time acquired chunked models. These models feel primitive and are manipulated almost entirely outside of conscious awareness⁴.

Due to chunking, models become ever more complex and layered as ones experience in a domain grows. At first you start off knowing close to nothing and your models are built largely from primordial concepts or models in other areas that don’t really apply. These primordial concepts then get compressed and become a larger conceptual unit (a new model). Over time this new model then gets incorporated along with several other models into another new model and each time this happens it is a compression so some information is lost. This has to happen or the model would keep getting bigger and bigger and, therefore, less useful as they would no longer suit our human scale level of thinking.

After each new layer is created, we lose some awareness of the models that took part in the creation of the chunk. This means that a chunk becomes increasingly tacit and like a blackbox overtime. When these chunks get complex enough that they have layers of hierarchy , that is, they become compressions of many chunks, the awareness of the models that made the chunk are often lost completely. Sometimes they can be dug up through some unpacking and decompression. However, decompression in the brain is not like that of a computer it is itself a construction, so we cannot be sure if the decompression is technically accurate even if it may seem to be correct.

Compression can occur not only across instances in the same domain, but also across similar instances in different domains. This process is called blending⁵ and is the basis of analogy and metaphor.

Blending is when a new model is created from combining models or modeling the pattern shared between models. In a blend each model is compressed by homogenizing their natures, experience and behaviors into a frame of prototypicality. So you pick out what is the same across the instances and create a new model that represents those aspects. This representation often goes beyond what is included in the constituent models. That is, it has an emergent structure because it is modeling a pattern that is the result of a combination of models. For example, blending is at work in a simple expressions such as “This surgeon is a butcher” which suggests that the surgeon is incompetent although incompetence is neither a feature of surgeons or of butchers.

The goal of blending is turning multiple models into one or understanding and experiencing one kind of thing in terms of another. Blending or compression across different domains is important because it allows the ability to apply insight from other domains . By creating new blended models, you are essentially finding similarities across models. This means that insights which worked for other models in the new blended model might also apply to the others or to the blended model in general.

Models enable human comprehension of complex systems⁶. Through blends, we use models to not only understand, but to also lead to new knowledge showing the connection between something unknown and something known; we learn to imagine new possibilities through the process of analogy.

A major cost of using models to create models is that it means that our models are largely products of ourselves. The information from which they are built, the choice of which model to use and the way in which those models are fleshed out are driven largely by properties of ourselves and the state we are in. Mental models also contain a lot of information that is implicit or deeper than that which we are consciously aware. Our mental models are proliferated with knowledge that has become ingrained and instinctive and, therefore, not easily acknowledged, communicated or shared. Because of the way that models subsume and build on each other a small instance of friction can become amplified as it cascades and grows through the multiple iterations of a model. That friction can also bleed into other models, especially if the model is used frequently, as this causes it to become part of the, often unrecognized or acknowledged bedrock from which future models are built.

This potentially comprised nature of our models means that using models as a basis for creating models, which is an innate process, is essentially precarious. It also means that our ability to debug or check our models for errors (friction) is also compromised.

There are a few different ways in which we attempt to discover the friction in our models which all have their own limitations and benefits. These include.

- Consistency checks. This process operates using the unconscious, but it is flawed in that it checks against the pre-existing potentially compromised models. When these checks fail, the result is an emotional response, e.g. mirth, doubt etc.
- A debugging process, similar in nature to problem solving, which requires consciousness. This is a process with strong limitations. It monopolizes large parts of the brains resources and cannot make use of the large and parallel operating machinery of the unconscious. It also takes a great amount of mental effort as it involves activating specific mental content and keeping activated against all the ongoing competition from perception for enough time to explore its implications and presuppositions.
- Conceptual models and externalizing the debugging to others or mechanical processes. If you can communicate your model, then other people can look for friction in the model in a less biased way. The limitation with this is that the contents of mental models are largely beyond conscious awareness and so are hard to communicate and communication itself is limited. When you communicate your model, another person would use your conceptual model to build a mental model of their own similar to yours. This can be a difficult task which in even the best case scenario will not produce a perfect duplication of your mental model. It would instead create a version that has been shaped by their own biases and mental models.
- Social signals. We not only model the world we model the people in it. In fact, a large part of the models we create are about other people, their intentions and what we believe are their mental models. We have evolved to feel emotions and exhibit behaviors when we discover flaws in the supposed models of others. For example, although a multifaceted emotion, mirth and its related behavior laughter could be considered in part to be a way to convey that you have discovered an epistemic error in either your mental models or those of others since mirth is largely a social emotion.
- Observing or interacting with the system itself. That is, checking whether predictions derived from the model align with what actually occurs. The limitation with this method is that perception is largely shaped by our existing models.

---

¹

We are likely to think that the perception of a spot in the visual field is caused simply and directly by the light coming to our eye from it, and we think of a black spot as being one from which no light is reflected. This is quite wrong, as it turns out. The amount of light reflected to our eye from a black letter in a newspaper headline outside in the sun is about twice the amount of light coming from the white paper in a dimly lit office, but we still see the letter as black and the paper as white under both conditions. (See Zeki 1993, Hubel 1995). We think the letter has the invariant property black, but that invariance is a neurological effect. Black in this case is not a primitive cause; it is a complex neurological effect. We integrate the effect - black - with its causes - the light striking our eye - to create the emergent meaning that black is primitively and intrinsically an invariant property of the letter.

- Hubel, David H. 1995. Eye, Brain, and Vision. New York: Scientific American Library.
- Zeki, Semir. A Vision of the Brain. Oxford: Blackwell, 1993.

The brain has an extremely difficult task in trying to represent its environment. It must do a passable job of a thrifty search without either lapsing into combinatorial explosion or failing to represent key elements. One method it uses to make progress in this task is spreading activation. Spreading activation means that activating one memory triggers associated items which when activated then trigger further memories. This process of activation spread out in a treelike formation. This process is not infinite, but is restricted by:

- Friction – each activated memory has less strength (ability to cause more activations) that its activator. Eventually the strength reaches a low enough point that it is not strong enough to activate any other mental content.
- Closure – this occurs when something about the content in some activation path actively closes off further exploration. An example of closure is when a chess player ignores his opponent's surviving bishop (tacitly assuming that the bishop does not require further consideration). Closure is necessarily risky as it is a process of excluding potential activation paths. This is distinct from simply not having found time to consider the bishop at all. The main power of spreading activation comes from the use of closure. Since friction is as good as content-blind, stopping the search for no reason at all other than running out of time or energy. Closure, in contrast, is teachable and adjustable by experience. We can think of it as a thrifty triage system, helping not-quite-blindly to allocate resources, by "selfishly looking for excuses" to terminate its own activation any time its local hunch is that the current task is unlikely to engage its talents productively.

Spreading activation is an ongoing process. People are constantly generating pertinent anticipations about the world. Such anticipations aren’t created through effortful enumeration of all the possibilities followed by their assesment. They are instead the result of spreading activation starting from current situation-pertinent thought or recollections. These anticipations largely help us navigate the world because it is often reasonable to expect future events to fall in line with our experiences and with the inferential anticipations we have had occasion to create. It is our good luck (thanks to evolution by natural selection) that these expectations happen to be, on the whole, the most relevant anticipations, out of an infinite space of logically possible thoughts. This relevance follows for the simple reason that these anticipations are most applicable to precisely the environment from which they are drawn.

Spreading activation, thus, leads largely to relevant that are activated in paths that are similar to those activated in previous situations that were also similar. Spreading activation also leads to:

- Models being built incrementally and being and revised constantly. Comprehension is always accomplished by a "holistic" attempt to integrate the information, from all sources, that has arrived in the brain up until that point, and that when further information arrives that can disambiguate an earlier piece of information, the model is adjusted accordingly.
- Models being built as soon as possible. During the process of comprehension, the mind does not wait passively until it has "enough" information in a buffer to complete the disambiguation of what it has so far received but rather attempts to disambiguate by assumption until proven otherwise. These predictions may be "educated" assumptions due to quite explicit noticing of a telling feature, local priming that makes one possibility appear more likely than another, or they may be due to a subliminally learned statistical regularity that suggests the likelihood of one meaning rather than another.

If the fleshing out or decompression of models was not done in an on demand manner, then there would be a very deep quandary as to just how much (and which) forethought the brain needs to perform. Remember, speed matters and needlessly computing all manner of thought is not a rapid or viable strategy. Therefore, along with the speed gained through the parallel processing of spreading activation the brain also decompresses models in a Just in Time (JIT) or on demand manner. JIT processing is an economic model of processing in which the computation is performed at the last possible moment and only when it is needed. It is performed on demand. The JIT activation still proceeds using spreading activation which means that the JIT activation is not random, but like with all other thoughts and models is linked to pertinent recent perceptions, desires or emotions.
JIT activation happens so fast that it leads to the frame illusion or the illusion that the model is already and always was decompressed. Comprehension, thought, and recall, as opposed to the more effortful problem solving, happen so fast that we seem to have instantaneous access to a number of elements about any situation as if all these details are already actively loaded into working memory. In reality, some details are strongly activated and some details will not be activated at all. Yet, all of these things are instantly accessible upon the slightest inquiry because of the brains capacity for JIT or on demand activation. As an example if you think of a restaurant, there may be wine glasses in your current restaurant model, but you won’t have specified whether they are glasses for red or for white until you specifically think about it.
When you learn of a fictional character entering a restaurant, spreading activation may create a model with tables and chairs, waiters and menus, and other customers, with some of these things appearing obligatory (part of the "very definition" of a restaurant) and others appearing as likely options, perhaps with highly favored "default" values which demand to be accepted without checking or ruling them out through further decompression or exploratory experience. This kind of activation accounts for the frequency of spurious "filling in" that is one of the main contributors to falsehood in our models. For example, when you are told that Tom and Bill are playing catch on the beach, there are several different models you may create. If you are then asked what kind of ball it was. The model would suggest that you have already thought of some statistical default-a baseball, football, or beach ball-and inserted it thoughtlessly (without noticing) in your model. On the other hand, you may not have thought of any kind of ball at all, but upon interrogation, your exceedingly nimble JIT activation would supply one so quickly that it would seem as if it had been there from the start, much like the default model value.
Spreading and on demand activation are not perfect and can make two types of epistemic mistakes:

- Filling in wrong values or epistemically committing too strongly to something. When a wrong epistemic commitment is discovered it is felt through the epistemic emotions of surprise or confusion. These emotions are felt more or less strongly depending in how epistemically committed you were to the content that is proven wrong. For example, let’s say in your model representing Tim and Bill playing catch you entered a relatively uncommitted default, i.e. you weakly "filled in", that they were playing catch with a tennis ball. When later it emerges that it was a beach ball, you may hardly notice the revision. On the other hand, if it turns out that Tom and Bill were playing catch with a live fish, this is bound to cause some surprise or confusion since you were somewhat strongly committed to the idea of the model containing a default (but generic) ball.
- Not being able to fill in values or epistemically commit to one idea. This experience is felt through the epistemic emotion of uncertainty. For example, if you were asked what beach Tim and Bill were playing catch, it is perfectly plausible for you to say that you don’t know which means you didn’t probably think of any specific beach to begin with and you aren’t ready to commit to a particular one upon interrogation. While it is safer to not epistemically commit, the brain for the sake of timeliness is much more prone to making the first error, i.e. epistemically committing or filling values too early.

²
While we experience a wide field of view, the eyes are incapable of capturing the world in that way. They cannot capture the world as, say, a camera does in a single snapshot view. They must construct that view by essentially scanning the world and passing that information to the brain for cerebral synthesis. This means that each and every scene looked at by a person is experienced as a composite image constructed from information that comes from not one, but a multitude of perspectives.
When we look at a painting, say, we do not take it in all at once. Our eyes flit around in sharp saccadic movements. They rapidly and constantly look at different parts of the painting. This is because the retina only has the very small foveal region that is dedicated to high-resolution analysis the rest of it, the majority of it, can only monitor the visual field with low-resolution. The low resolution (peripheral) parts of our vision are great for scanning objects of interest, e.g. prey, threats or movement, but once they have spotted a potential object of interest the eyes must focus in with central (foveal) vision to discern the details of the object. When we ‘look’ (foveate) at a certain location the retinal image is clear only in the small part picked up by the fovea and is very blurry in the surrounding areas. This foveal region is so small that when you look at a scene an arm's length away, the fovea picks up a field that is only about the size of your thumbnail.

Even with it picking up such a small region of vision the fovea dominates our visual perception and provides more than half of the input that comes from the eyes to the visual cortex of the brain. The fovea is constantly flitting around as the eyes undergo rapid movements to direct the fovea to various parts of a scene. Yet, in our subjective experience, the world seems to be stable, coherent and complete in front of us.

To overcome the limited resolution of the retinal image, the brain actively moves the eyes around to gather information and locate interesting parts of the scene. The brain then uses this information to construct a mental, three-dimensional model of the scene and to make inferences about the world based on that mental model.

³

Largely attributed to the work of Miller (1956, 1994), chunking refers to the process of organizing and grouping small units of information into larger clusters. The ability to chunk information:

- helps an individual remember more
- gives the individual a means of accessing the information that is ultimately stored in his or her memory,
- increases ‘‘the amount of information we can deal with’’ (Miller, 1956, p. 95).
  Miller suggested that this process is ongoing as we recode information constantly in an effort to assimilate new information with current knowledge. For instance, as we learn new information, if it sounds familiar or if it fits into an existing category we tend to remember and relate the new information to the existing category, creating powerful connections within the chunk.

⁴
Evolution has restricted consciousness in a lot of areas to only see the chunk. For example, in perception, sensation, arousal and immediate reactions to environmental threats global and immediate insight is the priority, that is seeing only the chunk is the priority. In these situations there is little evolutionary incentive to check step-by-step how that global insight was achieved. This is the reason why scientific ideas and the process underlying them took so long to develop.
Scientific ideas are hard to form and express. This is because:

- They cannot have ambiguity. When decompressing a scientific conceptual model, the result must always be the same. This is different to art which can lead to a range of different mental models. Unlike art, scientific truth does not in principle vary from culture to culture.
- They must be factual. The ideal for scientific models is truth which means that ultimately they are judged based on how they fare when faced with cold, hard, experimental facts.
- The innate indicators of what is meaningful and therefore innately seen as worthy of expression are not always applicable to scientific ideas. For example, beauty and simplicity, as in Occam’s razor, are useful indicators that a scientific idea is worth pursuing or on the right track, but they are not infallible. True ideas do not need to be either beautiful or simple.
- Math and science skills are largely acquired and have to be taught.
  The reason for the difficulty in expressing high quality scientific ideas can be summed up by saying that they are meant to state meaning, not express it.

- they are made up of ideas How many levels of structure are needed to explain the ‘Wikipedia’? How much would you have to explain to someone from 2000 years ago to get them to understand it? You would have to explain computers and networks and the web and an encyclopedia and publishing and who knows what else.

⁵
One way to think about blending or compression in general is in terms of what has been left out. Compressions are always selective and through that selectiveness they help to highlight associations and similarities.

A working definition of association is that it is a function of co-occurence in space, time or language, for example cars and petrol are associated. Similarity is a function of overlap of perceptual, functional, conceptual features. For example, cars and bikes are similar. Similarity and association are two great forces of mental organization with humans they are demonstrated through analogy (a selective form of similarity) and causation (a selective form of association).

Blending involves:

- Selective projection from different models and integration of those details into a new model
- Syncopation which means that all but a few key elements of the original models are eliminated. The blended model tends to maximize or prioritize information regarding the similarities or vital relations between the models that make up the blend.
- Scaling. In stories, a lifetime can be scaled down to a day. The time it takes to go from childhood to professional adult, all the long changes involved, and the long causal chain from childhood experience to adult capacity, are scaled down. Scaling, in contrast to syncopation, preserves the topology of what is compressed. For example, temporal scaling preserves ordering of events.

Knowledge is not just a matter of what is known, but also the manner in which it is represented -- and the re-representation of a problem or a domain can constitute an epistemic advance of a significant and distinctly creative type (Boden 1990, and 1987 chapter 11).
By looking at data in a new mode or domain, researchers are able to see it in different ways, sometimes bringing about a conceptual change that is dramatic enough to cause a â€œframe shift.â€ This means that high-level perceptual functions create new structures, thereby building new data representations.
Metaphors can change knowledge. Sometimes they contribute to creatively enhanced thinking and sometimes to new representations of old concepts.
(Gentner & Wolff, 2000, p. 297) give four mechanisms for knowledge transformation:

1. Knowledge selection: This is one of the principal ways that metaphors and analogies clarify their target domains: the highlighting of some information and filtering out of other information.
2. Projection: A transfer of meaning from the source domain to the target domain, including the transfer of inferences: These inferences provide a vision into future possibilities of a specific context based on already achieved outcomes in the source domain.
3. Re-representation: The issue on nonidentical structural correspondences is important for similarity-based models. Re-representation is a process of restructuring the similarities between the two domains in the upper level of conceptualization so that they are the same.
4. Restructuring: Knowledge transformation can also occur at the systems level. This notion goes back to the beginnings of cognitive science. Analogy is selective similarity, and structure mapping is a â€œtheory of human processing of analogy and similarityâ€ (Gentner & Jeziorski, 1993, p. 448). A model is always a partial mapping. Part of creating a successful model is the knowledge of what to filter out from the mapping process. It must be limited because including all information would be an uninteresting duplication of the original
5. Models Are Analogy Analogy is the process of thinking about relational patterns. A fundamental human achievement is the ability to pick out patterns and identify recurrences despite variations. Humans form concepts that abstract and express the patterns in a language system (Gentner et al., 2001, p. 2). The mode of reexpression, or representation, is frequently other than linguistic, e.g., visual or sonic. What we create by mapping relational data to a new domain or mode is a model. Because it is multimodal, it can be considered a â€œtranslation,â€ or a â€œtransductionâ€ in Kressâ€™s more detailed system (Kress, 2010, p. 124). Two basic kinds of models occur: descriptive models used to simplify phenomena and explanatory models that fill in knowledge gaps. Explanatory models are analogues of unknown processes and mechanisms. Perhaps explanatory and descriptive models are functionally different aspects of one model. One view of science is that a theoryâ€™s role is to find and map data from phenomena to intermediary models. 52 Chapter Two The resulting inferences that come out of this data mapping leads to new knowledge (HarrÃ© et al., 2000, p. 3).

⁶

The production of art could be described as similar in form to perceptualization. Perceptualization is the translation of data in a form that is not intelligible into a form that is, i.e. into one based on the senses.
Humans don’t have a single optimal representation of the problems they solve. They redescribe the information and represent it in different formats. This allows them to explore different representations and to use multiple problem solving strategies, from low-level systematic search to abstract reasoning.
Mental models or chunks of them that are worth expressing are similar to data in a form that is not intelligible in that they are often highly complex, layered and relatively inaccessible. So, converting a mental model into a conceptual model could be seen as similar to perceptualization which requires:

- Many different models to fully express what is required “Knowledge typically is organized along many thousands of dimensions, but a map with thousands of dimensions cannot be used effectively by humans. For this reason, domain visualizations and the ability to interact with knowledge and view it from a variety of perspectives play a critical role.” (Shiffrin & Börner, 2004, p. 5183)
  aIterations of models as each produced model becomes a resource that is used to create future models “Perceptualization shapes the way we think and communicate. The value of perceptualization goes beyond knowledge representation, thereby enabling knowledge creation and transfer” (Pauwels, 2006, p. viii).
  Writing or drawing on a piece of paper can enable thoughts that are not available without engaging the hands. This phenomenon is expressed simply in an exchange between the historian Charles Weiner and the scientist Richard Feynman: Weiner once remarked casually that [a batch of notes and sketches] represented "a record of [Feynman's] day-to-day work," and Feynman reacted sharply. "I actually did the work on the paper," he said. "Well," Weiner said, "the work was done in your head, but the record of it is still here." [Feynman's reply:] "No, it’s not a record, not really, its working. You have to work on paper and this is the paper. Okay?" (Clark, 2013, p. 258; Gleick, 1993, p. 409)

- Pauwels, Luc. (2006). Introduction: The Role of Visual Representation in the Production of Scientific Reality. In Luc Pauwels (Ed.), Visual Cultures of Science: Rethinking Representational Practiced in Knowledge building and Science Communication. Hanover: Dartmouth College Press.
- Shiffrin, Richard M. and Börner, Katy (Eds.) (2004). Mapping Knowledge Domains. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl_1).
